{"componentChunkName":"component---src-templates-doc-template-js","path":"/haystack-io/en/docs/readermd","result":{"data":{"markdownRemark":{"frontmatter":{"id":"readermd","title":"Reader"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"readermd","headings":[{"value":"Reader","depth":1},{"value":"Choosing the Right Model","depth":2},{"value":"Languages other than English","depth":2},{"value":"Deeper Dive: FARM vs Transformers","depth":2},{"value":"Deeper Dive: From Language Model to Haystack Reader","depth":2}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/usage/usage/reader.md","editPath":"usage/usage/reader.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Reader</h1>\n<p>The Reader, also known as Open-Domain QA systems in Machine Learning speak,\nis the core component that enables Haystack to find the answers that you need.\nHaystack’s Readers are:</p>\n<ul>\n<li>built on the latest transformer based language models</li>\n<li>strong in their grasp of semantics</li>\n<li>sensitive to syntactic structure</li>\n<li>state-of-the-art in QA tasks like SQuAD and Natural Questions</li>\n</ul>\n<p>FARM</p>\n<pre><code>model = \"deepset/roberta-base-squad2\"\nreader = FARMReader(model, use_gpu=True)\nfinder = Finder(reader, retriever)\n</code></pre>\n<p>Transformers</p>\n<pre><code>model = \"deepset/roberta-base-squad2\"\nreader = TransformersReader(model, use_gpu=1)\nfinder = Finder(reader, retriever)\n</code></pre>\n<p>While these models can work on CPU, it is recommended that they are run using GPUs to keep query times low.</p>\n<h2>Choosing the Right Model</h2>\n<p>In Haystack, you can start using pretrained QA models simply by providing its HuggingFace Model Hub name to the Reader.\nThe loading of model weights is handled by Haystack,\nand you have the option of using the QA pipeline from deepset FARM or HuggingFace Transformers (see FARM vs Transformers for details).</p>\n<p>Currently, there are a lot of different models out there and it can be rather overwhelming trying to pick the one that fits your use case.\nTo get you started, we have a few recommendations for you to try out.</p>\n<p>FARM</p>\n<p>RoBERTa (base)</p>\n<p><strong>An optimised variant of BERT and a great starting point.</strong></p>\n<pre><code>reader = FARMReader(\"deepset/roberta-base-squad2\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Strong all round model</li>\n<li><strong>Con</strong>: There are other models that are either faster or more accurate</li>\n</ul>\n<p>MiniLM</p>\n<p><strong>A cleverly distilled model that sacrifices a little accuracy for speed.</strong></p>\n<pre><code>reader = FARMReader(\"deepset/minilm-uncased-squad2\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Inference speed up to 50% faster than BERT base</li>\n<li><strong>Con</strong>: Still doesn’t match the best base sized models in accuracy</li>\n</ul>\n<p>ALBERT (XXL)</p>\n<p><strong>Large, powerful, SotA model.</strong></p>\n<pre><code>reader = FARMReader(\"ahotrod/albert_xxlargev1_squad2_512\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Better accuracy than any other open source model in QA</li>\n<li><strong>Con</strong>: The computational power needed make it impractical for most use cases</li>\n</ul>\n<p>Transformers</p>\n<p>RoBERTa (base)</p>\n<p><strong>An optimised variant of BERT and a great starting point.</strong></p>\n<pre><code>reader = TransformersReader(\"deepset/roberta-base-squad2\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Strong all round model</li>\n<li><strong>Con</strong>: There are other models that are either faster or more accurate</li>\n</ul>\n<p>MiniLM</p>\n<p><strong>A cleverly distilled model that sacrifices a little accuracy for speed.</strong></p>\n<pre><code>reader = TransformersReader(\"deepset/minilm-uncased-squad2\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Inference speed up to 50% faster than BERT base</li>\n<li><strong>Con</strong>: Still doesn’t match the best base sized models in accuracy</li>\n</ul>\n<p>ALBERT (XXL)</p>\n<p><strong>Large, powerful, SotA model.</strong></p>\n<pre><code>reader = TransformersReader(\"ahotrod/albert_xxlargev1_squad2_512\")\n</code></pre>\n<ul>\n<li><strong>Pro</strong>: Better accuracy than any other open source model in QA</li>\n<li><strong>Con</strong>: The computational power needed make it impractical for most use cases</li>\n</ul>\n<p><strong>All-rounder</strong>: In the class of base sized models trained on SQuAD, <strong>RoBERTa</strong> has shown better performance than BERT\nand can be capably handled by any machine equipped with a single NVidia V100 GPU.\nWe recommend this as the starting point for anyone wanting to create a performant and computationally reasonable instance of Haystack.</p>\n<p><strong>Built for Speed</strong>: If speed and GPU memory are more of a priority to you than accuracy,\nyou should try the MiniLM model.\nIt is a smaller model that is trained to mimic larger models through the distillation process,\nand it outperforms the BERT base on SQuAD even though it is about 40% smaller.</p>\n<!-- _comment: !! In our tests we found that it was XX% faster than BERT and ~X% better in perfomance. Compared to RoBERTa, it is only off by about X% absolute, -->\n<p><strong>State of the Art Accuracy</strong>: For most, <strong>ALBERT XXL</strong> will be too large to feasibly work with.\nBut if performance is your sole concern, and you have the computational resources,\nyou might like to try ALBERT XXL which has set SoTA performance on SQuAD 2.0.</p>\n<!-- _comment: !! How good is it? How much computation resource do you need to run it? !! -->\n<h2>Languages other than English</h2>\n<p>Haystack is also very well suited to open-domain QA on languages other than English.\nWhile models are comparatively more performant on English,\nthanks to a wealth of available English training data,\nthere are a couple QA models that are directly usable in Haystack.</p>\n<p>FARM</p>\n<p>French</p>\n<pre><code>reader = FARMReader(\"illuin/camembert-base-fquad\")\n</code></pre>\n<p>Italian</p>\n<pre><code>reader = FARMReader(\"mrm8488/bert-italian-finedtuned-squadv1-it-alfa\")\n</code></pre>\n<p>Zero-shot</p>\n<pre><code>reader = FARMReader(\"deepset/xlm-roberta-large-squad2\")\n</code></pre>\n<p>Transformers</p>\n<p>French</p>\n<pre><code>reader = TransformersReader(\"illuin/camembert-base-fquad\")\n</code></pre>\n<p>Italian</p>\n<pre><code>reader = TransformersReader(\"mrm8488/bert-italian-finedtuned-squadv1-it-alfa\")\n</code></pre>\n<p>Zero-shot</p>\n<pre><code>reader = TransformersReader(\"deepset/xlm-roberta-large-squad2\")\n</code></pre>\n<p>The <strong>French</strong> and <strong>Italian models</strong> are both monolingual langauge models trained on French and Italian versions of the SQuAD dataset\nand their authors report decent results in their model cards\n<a href=\"https://huggingface.co/illuin/camembert-base-fquad\">here</a> and <a href=\"https://huggingface.co/illuin/camembert-base-fquad\">here</a> .\nNote that there is also a <a href=\"https://huggingface.co/illuin/camembert-large-fquad\">large variant</a> of the French model available on the model hub.\nThere also exist Korean QA models on the model hub but their performance is not reported.</p>\n<p>The <strong>zero-shot model</strong> that is shown above is a <strong>multilingual XLM-RoBERTa Large</strong> that is trained on English SQuAD.\nIt is clear, from our <a href=\"https://huggingface.co/deepset/xlm-roberta-large-squad2#model_card\">evaluations</a>,\nthat the model has been able to transfer some of its English QA capabilities to other languages,\nbut still its performance lags behind that of the monolingual models.\nNonetheless, if there is not yet a monolingual model for your language and it is one of the 100 supported by XLM-RoBERTa,\nthis zero-shot model may serve as a decent first baseline.</p>\n<p>When using a Reader of any language, it’s important to ensure that the Retriever is also compatible.\nWhile sparse methods like BM25 and TF-IDF are language agnostic,\ndense method like Dense Passage Retrieval are trained for a particular language.</p>\n<!-- farm-vs-trans: -->\n<h2>Deeper Dive: FARM vs Transformers</h2>\n<p>Apart from the <strong>model weights</strong>, Haystack Readers contain all the components found in end-to-end open domain QA systems.\nThis includes <strong>tokenization</strong>, <strong>embedding computation</strong>, <strong>span prediction</strong> and <strong>candidate aggregation</strong>.\nWhile the handling of model weights is the same between the FARM and Transformers libraries, their QA pipelines differ in some ways.\nThe major points are:</p>\n<ul>\n<li>The <strong>TransformersReader</strong> will sometimes predict the same span twice while duplicates are removed in the <strong>FARMReader</strong></li>\n<li>The <strong>FARMReader</strong> currently uses the tokenizers from the HuggingFace Transformers library while the <strong>TransformersReader</strong> uses the tokenizers from the HuggingFace Tokenizers library</li>\n<li>Start and end logits are normalized per passage and multiplied in the <strong>TransformersReader</strong> while they are summed and not normalised in the <strong>FARMReader</strong></li>\n</ul>\n<p>If you’re interested in the finer details of these points, have a look at <a href=\"https://github.com/deepset-ai/haystack/issues/248#issuecomment-661977237\">this</a> GitHub comment.</p>\n<p>We see value in maintaining both kinds of Readers since Transformers is a very familiar library to many of Haystack’s users\nbut we at deepset can more easily update and optimise the FARM pipeline for speed and performance.</p>\n<!-- _comment: !! benchmarks !! -->\n<p>Haystack also has a close integration with FARM which means that you can further fine-tune your Readers on labelled data using a FARMReader.\nSee our tutorials for an end-to-end example or below for a shortened example.</p>\n<pre><code># Initialise Reader\nmodel = \"deepset/roberta-base-squad2\"\nreader = FARMReader(model)\n\n# Perform finetuning\ntrain_data = \"PATH/TO_YOUR/TRAIN_DATA\"\ntrain_filename = \"train.json\"\nsave_dir = \"finetuned_model\"\nreader.train(train_data, train_filename, save_dir=save_dir)\n\n# Load\nfinetuned_reader = FARMReader(save_dir)\n</code></pre>\n<h2>Deeper Dive: From Language Model to Haystack Reader</h2>\n<p>Language models form the core of most modern NLP systems and that includes the Readers in Haystack.\nThey build a general understanding of language when performing training tasks such as Masked Language Modeling or Replaced Token Detection\non large amounts of text.\nWell trained language models capture the word distribution in one or more languages\nbut more importantly, convert input text into a set of word vectors that capture elements of syntax and semantics.</p>\n<p>In order to convert a language model into a Reader model, it needs first to be trained on a Question Answering dataset.\nTo do so requires the addition of a question answering prediction head on top of the language model.\nThe task can be thought of as a token classification task where every input token is assigned a probability of being\neither the start or end token of the correct answer.\nIn cases where the answer is not contained within the passage, the prediction head is also expected to return a <code>no_answer</code> prediction.</p>\n<!-- _comment: !! Diagram of language model / prediction head !! -->\n<p>Since language models are limited in the number of tokens which they can process in a single forward pass,\na sliding window mechanism is implemented to handle variable length documents.\nThis functions by slicing the document into overlapping passages of (approximately) <code>max_seq_length</code>\nthat are each offset by <code>doc_stride</code> number of tokens.\nThese can be set when the Reader is initialized.</p>\n<p>FARM</p>\n<pre><code>reader = FARMReader(... max_seq_len=384, doc_stride=128 ...)\n</code></pre>\n<p>Transformers</p>\n<pre><code>reader = TransformersReader(... max_seq_len=384, doc_stride=128 ...\n</code></pre>\n<p>Predictions are made on each individual passage and the process of aggregation picks the best candidates across all passages.\nIf you’d like to learn more about what is happening behind the scenes, have a look at <a href=\"https://medium.com/deepset-ai/modern-question-answering-systems-explained-4d0913744097\">this</a> article.</p>\n<!-- _comment: !! Diagram from Blog !! -->"}},"staticQueryHashes":[]}