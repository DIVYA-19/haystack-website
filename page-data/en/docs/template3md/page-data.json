{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/template3md","result":{"data":{"markdownRemark":{"frontmatter":{"id":"template3md","title":"Tutorial 3"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Milvus","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Milvus","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"template3md","headings":[{"value":"Task: Build a Question Answering pipeline without Elasticsearch","depth":1},{"value":"Document Store","depth":2},{"value":"Cleaning & indexing documents","depth":2},{"value":"Initalize Retriever, Reader, & Finder","depth":2},{"value":"Retriever","depth":3},{"value":"Reader","depth":3},{"value":"Finder","depth":3},{"value":"Voilà! Ask a question!","depth":2}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/tutorials/tutorials/3.md","editPath":"tutorials/tutorials/3.md","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Task: Build a Question Answering pipeline without Elasticsearch</h1>\n<p>Haystack provides alternatives to Elasticsearch for developing quick\nprototypes.</p>\n<p>You can use an <code>InMemoryDocumentStore</code> or a\n<code>SQLDocumentStore</code>(with SQLite) as the document store.</p>\n<p>If you are interested in more feature-rich Elasticsearch, then please\nrefer to the Tutorial 1.</p>\n<pre><code># Install the latest release of Haystack in your own environment\n#! pip install farm-haystack\n\n# Install the latest master of Haystack and install the version of torch that works with the colab GPUs\n!pip install git+https://github.com/deepset-ai/haystack.git\n!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<pre><code>from haystack import Finder\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers\n</code></pre>\n<h2>Document Store</h2>\n<pre><code># In-Memory Document Store\nfrom haystack.database.memory import InMemoryDocumentStore\ndocument_store = InMemoryDocumentStore()\n</code></pre>\n<pre><code># SQLite Document Store\n# from haystack.database.sql import SQLDocumentStore\n# document_store = SQLDocumentStore(url=\"sqlite:///qa.db\")\n</code></pre>\n<h2>Cleaning &#x26; indexing documents</h2>\n<p>Haystack provides a customizable cleaning and indexing pipeline for\ningesting documents in Document Stores.</p>\n<p>In this tutorial, we download Wikipedia articles on Game of Thrones,\napply a basic cleaning function, and index them in Elasticsearch.</p>\n<pre><code># Let's first get some documents that we want to query\n# Here: 517 Wikipedia articles for Game of Thrones\ndoc_dir = \"data/article_txt_got\"\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# convert files to dicts containing documents that can be indexed to our datastore\n# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n# It must take a str as input, and return a str.\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n\n# We now have a list of dictionaries that we can write to our document store.\n# If your texts come from a different source (e.g. a DB), you can of course skip convert_files_to_dicts() and create the dictionaries yourself.\n# The default format here is: {\"name\": \"&#x3C;some-document-name>, \"text\": \"&#x3C;the-actual-text>\"}\n\n# Let's have a look at the first 3 entries:\nprint(dicts[:3])\n# Now, let's write the docs to our DB.\ndocument_store.write_documents(dicts)\n</code></pre>\n<h2>Initalize Retriever, Reader, &#x26; Finder</h2>\n<h3>Retriever</h3>\n<p>Retrievers help narrowing down the scope for the Reader to smaller units\nof text where a given question could be answered.</p>\n<p>With InMemoryDocumentStore or SQLDocumentStore, you can use the\nTfidfRetriever. For more retrievers, please refer to the tutorial-1.</p>\n<pre><code># An in-memory TfidfRetriever based on Pandas dataframes\n\nfrom haystack.retriever.sparse import TfidfRetriever\nretriever = TfidfRetriever(document_store=document_store)\n</code></pre>\n<h3>Reader</h3>\n<p>A Reader scans the texts returned by retrievers in detail and extracts\nthe k best answers. They are based on powerful, but slower deep learning\nmodels.</p>\n<p>Haystack currently supports Readers based on the frameworks FARM and\nTransformers. With both you can either load a local model or one from\nHugging Face’s model hub (<a href=\"https://huggingface.co/models\">https://huggingface.co/models</a>).</p>\n<p><strong>Here:</strong> a medium sized RoBERTa QA model using a Reader based on FARM\n(<a href=\"https://huggingface.co/deepset/roberta-base-squad2\">https://huggingface.co/deepset/roberta-base-squad2</a>)</p>\n<p><strong>Alternatives (Reader):</strong> TransformersReader (leveraging the\n<code>pipeline</code> of the Transformers package)</p>\n<p><strong>Alternatives (Models):</strong>\ne.g. “distilbert-base-uncased-distilled-squad” (fast) or\n“deepset/bert-large-uncased-whole-word-masking-squad2” (good accuracy)</p>\n<p><strong>Hint:</strong> You can adjust the model to return “no answer possible” with\nthe no<em>ans</em>boost. Higher values mean the model prefers “no answer\npossible”</p>\n<h4>FARMReader</h4>\n<pre><code># Load a  local model or any of the QA models on\n# Hugging Face's model hub (https://huggingface.co/models)\n\nreader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n</code></pre>\n<h4>TransformersReader</h4>\n<pre><code># Alternative:\n# reader = TransformersReader(model=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)\n</code></pre>\n<h3>Finder</h3>\n<p>The Finder sticks together reader and retriever in a pipeline to answer\nour actual questions.</p>\n<pre><code>finder = Finder(reader, retriever)\n</code></pre>\n<h2>Voilà! Ask a question!</h2>\n<pre><code># You can configure how many candidates the reader and retriever shall return\n# The higher top_k_retriever, the better (but also the slower) your answers.\nprediction = finder.get_answers(question=\"Who is the father of Arya Stark?\", top_k_retriever=10, top_k_reader=5)\n</code></pre>\n<pre><code># prediction = finder.get_answers(question=\"Who created the Dothraki vocabulary?\", top_k_reader=5)\n# prediction = finder.get_answers(question=\"Who is the sister of Sansa?\", top_k_reader=5)\n</code></pre>\n<pre><code>print_answers(prediction, details=\"minimal\")\n</code></pre>"}},"staticQueryHashes":[]}