{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/template6md","result":{"data":{"markdownRemark":{"frontmatter":{"id":"template6md","title":"Tutorial 6"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Milvus","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Milvus","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"template6md","headings":[{"value":"Better retrieval via “Dense Passage Retrieval”","depth":1},{"value":"Importance of Retrievers","depth":2},{"value":"Different types of Retrievers","depth":2},{"value":"Sparse","depth":3},{"value":"Dense","depth":3},{"value":"“Dense Passage Retrieval”","depth":2},{"value":"Prepare environment","depth":2},{"value":"Colab: Enable the GPU runtime","depth":3},{"value":"Make sure you have a GPU running","depth":1},{"value":"Install the latest release of Haystack in your own environment","depth":1},{"value":"! pip install farm-haystack","depth":1},{"value":"Install the latest master of Haystack and install the version of torch that works with the colab GPUs","depth":1},{"value":"Let's first get some files that we want to use","depth":1},{"value":"Convert files to dicts","depth":1},{"value":"Now, let's write the dicts containing documents to our DB.","depth":1},{"value":"Important:","depth":1},{"value":"Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all","depth":1},{"value":"previously indexed documents and update their embedding representation.","depth":1},{"value":"While this can be a time consuming operation (depending on corpus size), it only needs to be done once.","depth":1},{"value":"At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.","depth":1},{"value":"Load a  local model or any of the QA models on","depth":1},{"value":"Hugging Face's model hub (https://huggingface.co/models)","depth":1},{"value":"You can configure how many candidates the reader and retriever shall return","depth":1},{"value":"The higher topkretriever, the better (but also the slower) your answers.","depth":1},{"value":"prediction = finder.getanswers(question=\"Who is the father of Arya Stark?\", topkretriever=10, topk_reader=5)","depth":1},{"value":"prediction = finder.getanswers(question=\"Who is the sister of Sansa?\", topkretriever=10, topk_reader=5)","depth":1}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/tutorials/tutorials/6.md","editPath":"tutorials/tutorials/6.md","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Better retrieval via “Dense Passage Retrieval”</h1>\n<h2>Importance of Retrievers</h2>\n<p>The Retriever has a huge impact on the performance of our overall search\npipeline.</p>\n<h2>Different types of Retrievers</h2>\n<h3>Sparse</h3>\n<p>Family of algorithms based on counting the occurences of words\n(bag-of-words) resulting in very sparse vectors with length = vocab\nsize.</p>\n<p><strong>Examples</strong>: BM25, TF-IDF</p>\n<p><strong>Pros</strong>: Simple, fast, well explainable</p>\n<p><strong>Cons</strong>: Relies on exact keyword matches between query and text</p>\n<h3>Dense</h3>\n<p>These retrievers use neural network models to create “dense” embedding\nvectors. Within this family there are two different approaches:</p>\n<ol>\n<li>Single encoder: Use a <strong>single model</strong> to embed both query and\npassage.</li>\n<li>Dual-encoder: Use <strong>two models</strong>, one to embed the query and one to\nembed the passage</li>\n</ol>\n<p>Recent work suggests that dual encoders work better, likely because they\ncan deal better with the different nature of query and passage (length,\nstyle, syntax …).</p>\n<p><strong>Examples</strong>: REALM, DPR, Sentence-Transformers</p>\n<p><strong>Pros</strong>: Captures semantinc similarity instead of “word matches”\n(e.g. synonyms, related topics …)</p>\n<p><strong>Cons</strong>: Computationally more heavy, initial training of model</p>\n<h2>“Dense Passage Retrieval”</h2>\n<p>In this Tutorial, we want to highlight one “Dense Dual-Encoder” called\nDense Passage Retriever. It was introdoced by Karpukhin et al. (2020,\n<a href=\"https://arxiv.org/abs/2004.04906\">https://arxiv.org/abs/2004.04906</a>.</p>\n<p>Original Abstract:</p>\n<p><em>“Open-domain question answering relies on efficient passage retrieval\nto select candidate contexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de facto method. In this work,\nwe show that retrieval can be practically implemented using dense\nrepresentations alone, where embeddings are learned from a small number\nof questions and passages by a simple dual-encoder framework. When\nevaluated on a wide range of open-domain QA datasets, our dense\nretriever outperforms a strong Lucene-BM25 system largely by 9%-19%\nabsolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple\nopen-domain QA benchmarks.”</em></p>\n<p>Paper: <a href=\"https://arxiv.org/abs/2004.04906\">https://arxiv.org/abs/2004.04906</a>Original Code: <a href=\"https://fburl.com/qa-dpr\">https://fburl.com/qa-dpr</a><em>Use this</em>\n<a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb\">link</a>\n<em>to open the notebook in Google Colab.</em></p>\n<h2>Prepare environment</h2>\n<h3>Colab: Enable the GPU runtime</h3>\n<p>Make sure you enable the GPU runtime to experience decent speed in\nthis tutorial.<strong>Runtime -> Change Runtime type -> Hardware accelerator -> GPU</strong>`<code></code></p>\n<h1>Make sure you have a GPU running</h1>\n<p>!nvidia-smi</p>\n<pre><code></code></pre>\n<h1>Install the latest release of Haystack in your own environment</h1>\n<h1>! pip install farm-haystack</h1>\n<h1>Install the latest master of Haystack and install the version of torch that works with the colab GPUs</h1>\n<p>!pip install git+<a href=\"https://github.com/deepset-ai/haystack.git\">https://github.com/deepset-ai/haystack.git</a>\n!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f <a href=\"https://download.pytorch.org/whl/torch_stable.html\">https://download.pytorch.org/whl/torch_stable.html</a></p>\n<pre><code></code></pre>\n<p>from haystack import Finder\nfrom haystack.indexing.cleaning import clean<em>wiki</em>text\nfrom haystack.indexing.utils import convert<em>files</em>to<em>dicts, fetch</em>archive<em>from</em>http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers</p>\n<pre><code>## Document Store\n\nFAISS is a library for efficient similarity search on a cluster of dense\nvectors. The `FAISSDocumentStore` uses a SQL(SQLite in-memory be\ndefault) database under-the-hood to store the document text and other\nmeta data. The vector embeddings of the text are indexed on a FAISS\nIndex that later is queried for searching answers.\n</code></pre>\n<p>from haystack.database.faiss import FAISSDocumentStore</p>\n<p>document_store = FAISSDocumentStore()</p>\n<pre><code>## Cleaning &#x26; indexing documents\n\nSimilarly to the previous tutorials, we download, convert and index some\nGame of Thrones articles to our DocumentStore\n</code></pre>\n<h1>Let's first get some files that we want to use</h1>\n<p>doc<em>dir = \"data/article</em>txt<em>got\"\ns3</em>url = \"<a href=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\">https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip</a>\"\nfetch<em>archive</em>from<em>http(url=s3</em>url, output<em>dir=doc</em>dir)</p>\n<h1>Convert files to dicts</h1>\n<p>dicts = convert<em>files</em>to<em>dicts(dir</em>path=doc<em>dir, clean</em>func=clean<em>wiki</em>text, split_paragraphs=True)</p>\n<h1>Now, let's write the dicts containing documents to our DB.</h1>\n<p>document<em>store.write</em>documents(dicts)</p>\n<pre><code>## Initalize Retriever, Reader, &#x26; Finder\n\n### Retriever\n\n**Here:** We use a `DensePassageRetriever`\n\n**Alternatives:**\n\n\n* The `ElasticsearchRetriever`with custom queries (e.g. boosting)\nand filters\n\n\n* Use `EmbeddingRetriever` to find candidate documents based on the\nsimilarity of embeddings (e.g. created via Sentence-BERT)\n\n\n* Use `TfidfRetriever` in combination with a SQL or InMemory Document\nstore for simple prototyping and debugging\n</code></pre>\n<p>from haystack.retriever.dense import DensePassageRetriever\nretriever = DensePassageRetriever(document<em>store=document</em>store,\nquery<em>embedding</em>model=\"facebook/dpr-question<em>encoder-single-nq-base\",\npassage</em>embedding<em>model=\"facebook/dpr-ctx</em>encoder-single-nq-base\",\nuse<em>gpu=True,\nembed</em>title=True,\nmax<em>seq</em>len=256,\nbatch<em>size=16,\nremove</em>sep<em>tok</em>from<em>untitled</em>passages=True)</p>\n<h1>Important:</h1>\n<h1>Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all</h1>\n<h1>previously indexed documents and update their embedding representation.</h1>\n<h1>While this can be a time consuming operation (depending on corpus size), it only needs to be done once.</h1>\n<h1>At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.</h1>\n<p>document<em>store.update</em>embeddings(retriever)</p>\n<pre><code>### Reader\n\nSimilar to previous Tutorials we now initalize our reader.\n\nHere we use a FARMReader with the *deepset/roberta-base-squad2* model\n(see: [https://huggingface.co/deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2))\n\n#### FARMReader\n</code></pre>\n<h1>Load a  local model or any of the QA models on</h1>\n<h1>Hugging Face's model hub (<a href=\"https://huggingface.co/models\">https://huggingface.co/models</a>)</h1>\n<p>reader = FARMReader(model<em>name</em>or<em>path=\"deepset/roberta-base-squad2\", use</em>gpu=True)</p>\n<pre><code>### Finder\n\nThe Finder sticks together reader and retriever in a pipeline to answer\nour actual questions.\n</code></pre>\n<p>finder = Finder(reader, retriever)</p>\n<pre><code>## Voilà! Ask a question!\n</code></pre>\n<h1>You can configure how many candidates the reader and retriever shall return</h1>\n<h1>The higher top<em>k</em>retriever, the better (but also the slower) your answers.</h1>\n<p>prediction = finder.get<em>answers(question=\"Who created the Dothraki vocabulary?\", top</em>k<em>retriever=10, top</em>k_reader=5)</p>\n<h1>prediction = finder.get<em>answers(question=\"Who is the father of Arya Stark?\", top</em>k<em>retriever=10, top</em>k_reader=5)</h1>\n<h1>prediction = finder.get<em>answers(question=\"Who is the sister of Sansa?\", top</em>k<em>retriever=10, top</em>k_reader=5)</h1>\n<pre><code></code></pre>\n<p>print_answers(prediction, details=\"minimal\")</p>\n<pre><code></code></pre>"}},"staticQueryHashes":[]}