{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/tutorial2md","result":{"data":{"markdownRemark":{"frontmatter":{"id":"tutorial2md","title":"Tutorial 2"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"tutorial2md","headings":[{"value":"Fine-tuning a model on your own data","depth":1},{"value":"Create Training Data","depth":2},{"value":"Fine-tune your model","depth":2}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/tutorials/tutorials/2.md","editPath":"tutorials/tutorials/2.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Fine-tuning a model on your own data</h1>\n<p>For many use cases it is sufficient to just use one of the existing\npublic models that were trained on SQuAD or other public QA datasets\n(e.g. Natural Questions). However, if you have domain-specific\nquestions, fine-tuning your model on custom examples will very likely\nboost your performance. While this varies by domain, we saw that ~ 2000\nexamples can easily increase performance by +5-20%.</p>\n<p>This tutorial shows you how to fine-tune a pretrained model on your own\ndataset.</p>\n<pre><code># Install the latest release of Haystack in your own environment\n#! pip install farm-haystack\n\n# Install the latest master of Haystack and install the version of torch that works with the colab GPUs\n!pip install git+https://github.com/deepset-ai/haystack.git\n!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<pre><code>from haystack.reader.farm import FARMReader\n</code></pre>\n<h2>Create Training Data</h2>\n<p>There are two ways to generate training data</p>\n<ol>\n<li><strong>Annotation</strong>: You can use the <a href=\"https://github.com/deepset-ai/haystack#labeling-tool\">annotation\ntool</a> to\nlabel your data, i.e. highlighting answers to your questions in a\ndocument. The tool supports structuring your workflow with\norganizations, projects, and users. The labels can be exported in\nSQuAD format that is compatible for training with Haystack.</li>\n<li><strong>Feedback</strong>: For production systems, you can collect training data\nfrom direct user feedback via Haystack’s <a href=\"https://github.com/deepset-ai/haystack#rest-api\">REST API\ninterface</a>. This\nincludes a customizable user feedback API for providing feedback on\nthe answer returned by the API. The API provides a feedback export\nendpoint to obtain the feedback data for fine-tuning your model\nfurther.</li>\n</ol>\n<h2>Fine-tune your model</h2>\n<p>Once you have collected training data, you can fine-tune your base\nmodels. We initialize a reader as a base model and fine-tune it on our\nown custom dataset (should be in SQuAD-like format). We recommend using\na base model that was trained on SQuAD or a similar QA dataset before to\nbenefit from Transfer Learning effects.</p>\n<p><strong>Recommendation</strong>: Run training on a GPU. If you are using Colab:\nEnable this in the menu “Runtime” > “Change Runtime type” > Select “GPU”\nin dropdown. Then change the <code>use_gpu</code> arguments below to <code>True</code></p>\n<pre><code>reader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=True)\ntrain_data = \"data/squad20\"\n# train_data = \"PATH/TO_YOUR/TRAIN_DATA\"\nreader.train(data_dir=train_data, train_filename=\"dev-v2.0.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")\n</code></pre>\n<pre><code># Saving the model happens automatically at the end of training into the `save_dir` you specified\n# However, you could also save a reader manually again via:\nreader.save(directory=\"my_model\")\n</code></pre>\n<pre><code># If you want to load it at a later point, just do:\nnew_reader = FARMReader(model_name_or_path=\"my_model\")\n</code></pre>"}},"staticQueryHashes":[]}