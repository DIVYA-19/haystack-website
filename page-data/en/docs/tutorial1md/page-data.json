{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/tutorial1md","result":{"data":{"markdownRemark":{"frontmatter":{"id":"tutorial1md","title":"Tutorial 1"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"tutorial1md","headings":[{"value":"Task: Question Answering for Game of Thrones","depth":1},{"value":"Document Store","depth":2},{"value":"Start an Elasticsearch server","depth":3},{"value":"Cleaning & indexing documents","depth":2},{"value":"Initalize Retriever, Reader, & Finder","depth":2},{"value":"Retriever","depth":3},{"value":"Reader","depth":3},{"value":"Finder","depth":3},{"value":"Voilà! Ask a question!","depth":2}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/tutorials/tutorials/1.md","editPath":"tutorials/tutorials/1.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Task: Question Answering for Game of Thrones</h1>\n<p>Question Answering can be used in a variety of use cases. A very common\none: Using it to navigate through complex knowledge bases or long\ndocuments (“search setting”).</p>\n<p>A “knowledge base” could for example be your website, an internal wiki\nor a collection of financial reports. In this tutorial we will work on a\nslightly different domain: “Game of Thrones”.</p>\n<p>Let’s see how we can use a bunch of Wikipedia articles to answer a\nvariety of questions about the marvellous seven kingdoms…</p>\n<p><em>Use this</em>\n<a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb\">link</a>\n<em>to open the notebook in Google Colab.</em></p>\n<pre><code># Install the latest release of Haystack in your own environment\n#! pip install farm-haystack\n\n# Install the latest master of Haystack and install the version of torch that works with the colab GPUs\n!pip install git+https://github.com/deepset-ai/haystack.git\n!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<pre><code>from haystack import Finder\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers\n</code></pre>\n<h2>Document Store</h2>\n<p>Haystack finds answers to queries within the documents stored in a\n<code>DocumentStore</code>. The current implementations of <code>DocumentStore</code>\ninclude <code>ElasticsearchDocumentStore</code>, <code>FAISSDocumentStore</code>,\n<code>SQLDocumentStore</code>, and <code>InMemoryDocumentStore</code>.</p>\n<p><strong>Here:</strong> We recommended Elasticsearch as it comes preloaded with\nfeatures like <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html\">full-text\nqueries</a>,\n<a href=\"https://www.elastic.co/elasticon/conf/2016/sf/improved-text-scoring-with-bm25\">BM25\nretrieval</a>,\nand <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.6/dense-vector.html\">vector storage for text\nembeddings</a>.</p>\n<p><strong>Alternatives:</strong> If you are unable to setup an Elasticsearch instance,\nthen follow the <a href=\"https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb\">Tutorial\n3</a>\nfor using SQL/InMemory document stores.</p>\n<p><strong>Hint</strong>: This tutorial creates a new document store instance with\nWikipedia articles on Game of Thrones. However, you can configure\nHaystack to work with your existing document stores.</p>\n<h3>Start an Elasticsearch server</h3>\n<p>You can start Elasticsearch on your local machine instance using Docker.\nIf Docker is not readily available in your environment (eg., in Colab\nnotebooks), then you can manually download and execute Elasticsearch\nfrom source.</p>\n<pre><code># Recommended: Start Elasticsearch using Docker\n#! docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.6.2\n</code></pre>\n<pre><code># In Colab / No Docker environments: Start Elasticsearch from source\n! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.6.2\n\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\nes_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n                   stdout=PIPE, stderr=STDOUT,\n                   preexec_fn=lambda: os.setuid(1)  # as daemon\n                  )\n# wait until ES has started\n! sleep 30\n</code></pre>\n<pre><code># Connect to Elasticsearch\n\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\ndocument_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")\n</code></pre>\n<h2>Cleaning &#x26; indexing documents</h2>\n<p>Haystack provides a customizable cleaning and indexing pipeline for\nloading documents into Document Stores.</p>\n<p>In this tutorial, we download Wikipedia articles about Game of Thrones,\napply a basic cleaning function, and index them in Elasticsearch.</p>\n<pre><code># Let's first fetch some documents that we want to query\n# Here: 517 Wikipedia articles for Game of Thrones\ndoc_dir = \"data/article_txt_got\"\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# Convert files to dicts\n# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n# It must take a str as input, and return a str.\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n\n# We now have a list of dictionaries that we can write to our document store.\n# If your texts come from a different source (e.g. a DB), you can of course skip convert_files_to_dicts() and create the dictionaries yourself.\n# The default format here is:\n# {\n#    'text': \"&#x3C;DOCUMENT_TEXT_HERE>\",\n#    'meta': {'name': \"&#x3C;DOCUMENT_NAME_HERE>\", ...}\n#}\n# (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and\n# can be accessed later for filtering or shown in the responses of the Finder)\n\n# Let's have a look at the first 3 entries:\nprint(dicts[:3])\n\n# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(dicts)\n</code></pre>\n<h2>Initalize Retriever, Reader, &#x26; Finder</h2>\n<h3>Retriever</h3>\n<p>Retrievers help narrowing down the scope for the Reader to smaller units\nof text where a given question could be answered. They use some simple\nbut fast algorithm.</p>\n<p><strong>Here:</strong> We use Elasticsearch’s default BM25 algorithm</p>\n<p><strong>Alternatives:</strong></p>\n<ul>\n<li>Customize the <code>ElasticsearchRetriever</code>with custom queries\n(e.g. boosting) and filters</li>\n<li>Use <code>TfidfRetriever</code> in combination with a SQL or InMemory Document\nstore for simple prototyping and debugging</li>\n<li>Use <code>EmbeddingRetriever</code> to find candidate documents based on the\nsimilarity of embeddings (e.g. created via Sentence-BERT)</li>\n<li>Use <code>DensePassageRetriever</code> to use different embedding models for\npassage and query (see Tutorial 6)</li>\n</ul>\n<pre><code>from haystack.retriever.sparse import ElasticsearchRetriever\nretriever = ElasticsearchRetriever(document_store=document_store)\n</code></pre>\n<pre><code># Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes with SQLite document store.\n\n# from haystack.retriever.sparse import TfidfRetriever\n# retriever = TfidfRetriever(document_store=document_store)\n</code></pre>\n<h3>Reader</h3>\n<p>A Reader scans the texts returned by retrievers in detail and extracts\nthe k best answers. They are based on powerful, but slower deep learning\nmodels.</p>\n<p>Haystack currently supports Readers based on the frameworks FARM and\nTransformers. With both you can either load a local model or one from\nHugging Face’s model hub (<a href=\"https://huggingface.co/models\">https://huggingface.co/models</a>).</p>\n<p><strong>Here:</strong> a medium sized RoBERTa QA model using a Reader based on FARM\n(<a href=\"https://huggingface.co/deepset/roberta-base-squad2\">https://huggingface.co/deepset/roberta-base-squad2</a>)</p>\n<p><strong>Alternatives (Reader):</strong> TransformersReader (leveraging the\n<code>pipeline</code> of the Transformers package)</p>\n<p><strong>Alternatives (Models):</strong>\ne.g. “distilbert-base-uncased-distilled-squad” (fast) or\n“deepset/bert-large-uncased-whole-word-masking-squad2” (good accuracy)</p>\n<p><strong>Hint:</strong> You can adjust the model to return “no answer possible” with\nthe no<em>ans</em>boost. Higher values mean the model prefers “no answer\npossible”</p>\n<h4>FARMReader</h4>\n<pre><code># Load a  local model or any of the QA models on\n# Hugging Face's model hub (https://huggingface.co/models)\n\nreader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n</code></pre>\n<h4>TransformersReader</h4>\n<pre><code># Alternative:\n# reader = TransformersReader(model=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)\n</code></pre>\n<h3>Finder</h3>\n<p>The Finder sticks together reader and retriever in a pipeline to answer\nour actual questions.</p>\n<pre><code>finder = Finder(reader, retriever)\n</code></pre>\n<h2>Voilà! Ask a question!</h2>\n<pre><code># You can configure how many candidates the reader and retriever shall return\n# The higher top_k_retriever, the better (but also the slower) your answers.\nprediction = finder.get_answers(question=\"Who is the father of Arya Stark?\", top_k_retriever=10, top_k_reader=5)\n</code></pre>\n<pre><code># prediction = finder.get_answers(question=\"Who created the Dothraki vocabulary?\", top_k_reader=5)\n# prediction = finder.get_answers(question=\"Who is the sister of Sansa?\", top_k_reader=5)\n</code></pre>\n<pre><code>print_answers(prediction, details=\"minimal\")\n</code></pre>"}},"staticQueryHashes":[]}