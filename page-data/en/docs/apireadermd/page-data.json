{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/apireadermd","result":{"data":{"markdownRemark":{"frontmatter":{"id":"apireadermd","title":"Reader"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"apireadermd","headings":[{"value":"Reader","depth":1},{"value":"Base","depth":2},{"value":"class haystack.reader.base.BaseReader()","depth":3},{"value":"FARM","depth":2},{"value":"class haystack.reader.farm.FARMReader(modelnameorpath: Unionstr, pathlib.Path, contextwindowsize: int = 150, batchsize: int = 50, usegpu: bool = True, noansboost: Optionalint = None, topkpercandidate: int = 3, topkpersample: int = 1, numprocesses: Optionalint = None, maxseqlen: int = 256, doc_stride: int = 128)","depth":3},{"value":"Transformers","depth":2},{"value":"class haystack.reader.transformers.TransformersReader(model: str = 'distilbert-base-uncased-distilled-squad', tokenizer: str = 'distilbert-base-uncased', contextwindowsize: int = 30, usegpu: int = 0, nbestperpassage: int = 2)","depth":3}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/api/api/reader.md","editPath":"api/api/reader.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Reader</h1>\n<h2>Base</h2>\n<h3>class haystack.reader.base.BaseReader()</h3>\n<p>Bases: <code>abc.ABC</code></p>\n<h2>FARM</h2>\n<h3>class haystack.reader.farm.FARMReader(model<em>name</em>or<em>path: Union[str, pathlib.Path], context</em>window<em>size: int = 150, batch</em>size: int = 50, use<em>gpu: bool = True, no</em>ans<em>boost: Optional[int] = None, top</em>k<em>per</em>candidate: int = 3, top<em>k</em>per<em>sample: int = 1, num</em>processes: Optional[int] = None, max<em>seq</em>len: int = 256, doc_stride: int = 128)</h3>\n<p>Bases: <code>haystack.reader.base.BaseReader</code></p>\n<p>Transformer based model for extractive Question Answering using the FARM framework (<a href=\"https://github.com/deepset-ai/FARM\">https://github.com/deepset-ai/FARM</a>).\nWhile the underlying model can vary (BERT, Roberta, DistilBERT …) the interface remains the same.</p>\n<p>With a FARMReader, you can:</p>\n<pre><code>* directly get predictions via predict()\n\n\n* fine-tune the model on QA data via train()\n</code></pre>\n<h4>_<em>init__(model</em>name<em>or</em>path: Union[str, pathlib.Path], context<em>window</em>size: int = 150, batch<em>size: int = 50, use</em>gpu: bool = True, no<em>ans</em>boost: Optional[int] = None, top<em>k</em>per<em>candidate: int = 3, top</em>k<em>per</em>sample: int = 1, num<em>processes: Optional[int] = None, max</em>seq<em>len: int = 256, doc</em>stride: int = 128)</h4>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>model<em>name</em>or_path</strong> – directory of a saved model or the name of a public model:</li>\n<li>‘bert-base-cased’</li>\n<li>‘deepset/bert-base-cased-squad2’</li>\n<li>‘deepset/bert-base-cased-squad2’</li>\n<li>‘distilbert-base-uncased-distilled-squad’\n….\nSee <a href=\"https://huggingface.co/models\">https://huggingface.co/models</a> for full list of available models.</li>\n<li><strong>context<em>window</em>size</strong> – The size, in characters, of the window around the answer span that is used when displaying the context around the answer.</li>\n<li><strong>batch_size</strong> – Number of samples the model receives in one batch for inference\nMemory consumption is much lower in inference mode. Recommendation: increase the batch size to a value so only a single batch is used.</li>\n<li><strong>use_gpu</strong> – Whether to use GPU (if available)</li>\n<li><strong>no<em>ans</em>boost</strong> – How much the no_answer logit is boosted/increased.\nPossible values: None (default) = disable returning “no answer” predictions</li>\n</ul>\n<blockquote>\n<p>Negative = lower chance of “no answer” being predicted\nPositive = increase chance of “no answer”</p>\n</blockquote>\n<ul>\n<li>\n<p><strong>top<em>k</em>per_candidate</strong> – How many answers to extract for each candidate doc that is coming from the retriever (might be a long text).</p>\n<p>Note: - This is not the number of “final answers” you will receive\n(see top<em>k in FARMReader.predict() or Finder.get</em>answers() for that)</p>\n<ul>\n<li>FARM includes no_answer in the sorted list of predictions</li>\n</ul>\n</li>\n<li><strong>top<em>k</em>per_sample</strong> – How many answers to extract from each small text passage that the model can\nprocess at once (one “candidate doc” is usually split into many smaller “passages”).\nYou usually want a very small value here, as it slows down inference and you\ndon’t gain much of quality by having multiple answers from one passage.</li>\n</ul>\n<blockquote>\n<blockquote>\n<p>Note: - This is not the number of “final answers” you will receive\n(see top<em>k in FARMReader.predict() or Finder.get</em>answers() for that)</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<pre><code>* FARM includes no_answer in the sorted list of predictions\n</code></pre>\n</blockquote>\n<ul>\n<li><strong>num_processes</strong> (<em>int</em>) – the number of processes for multiprocessing.Pool. Set to value of 0 to disable\nmultiprocessing. Set to None to let Inferencer determine optimum number. If you\nwant to debug the Language Model, you might need to disable multiprocessing!</li>\n<li><strong>max<em>seq</em>len</strong> – max sequence length of one input text for the model</li>\n<li><strong>doc_stride</strong> – length of striding window for splitting long texts (used if len(text) > max<em>seq</em>len)</li>\n</ul>\n</li>\n</ul>\n<h4>classmethod convert<em>to</em>onnx(model<em>name</em>or<em>path, opset</em>version: int = 11, optimize_for: Optional[str] = None)</h4>\n<p>Convert a PyTorch BERT model to ONNX format and write to ./onnx-export dir. The converted ONNX model\ncan be loaded with in the FARMReader using the export path as model<em>name</em>or_path param.</p>\n<p>Usage:</p>\n<pre><code>```python\n>>> from haystack.reader.farm import FARMReader\n>>> FARMReader.convert_to_onnx(model_name_or_path=\"deepset/bert-base-cased-squad2\", optimize_for=\"gpu_tensor_core\")\n>>> FARMReader(model_name_or_path=Path(\"onnx-export\"))\n```\n</code></pre>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>opset_version</strong> – ONNX opset version</li>\n<li><strong>optimize_for</strong> – optimize the exported model for a target device. Available options\nare “gpu<em>tensor</em>core” (GPUs with tensor core like V100 or T4),\n“gpu<em>without</em>tensor_core” (most other GPUs), and “cpu”.</li>\n</ul>\n</li>\n</ul>\n<h4>eval(document<em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, device: str, label</em>index: str = 'feedback', doc<em>index: str = 'eval</em>document', label<em>origin: str = 'gold</em>label')</h4>\n<p>Performs evaluation on evaluation documents in Elasticsearch DocumentStore.</p>\n<p>Returns a dict containing the following metrics:</p>\n<pre><code>* “EM”: Proportion of exact matches of predicted answers with their corresponding correct answers\n\n\n* “f1”: Average overlap between predicted answers and their corresponding correct answers\n\n\n* “top_n_accuracy”: Proportion of predicted answers that match with correct answer\n</code></pre>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>document_store</strong> (<em>ElasticsearchDocumentStore</em>) – The ElasticsearchDocumentStore containing the evaluation documents</li>\n<li><strong>device</strong> (<em>str</em>) – The device on which the tensors should be processed. Choose from “cpu” and “cuda”.</li>\n<li><strong>label_index</strong> (<em>str</em>) – Elasticsearch index where labeled questions are stored</li>\n<li><strong>doc_index</strong> (<em>str</em>) – Elasticsearch index where documents that are used for evaluation are stored</li>\n</ul>\n</li>\n</ul>\n<h4>eval<em>on</em>file(data<em>dir: str, test</em>filename: str, device: str)</h4>\n<p>Performs evaluation on a SQuAD-formatted file.</p>\n<p>Returns a dict containing the following metrics:</p>\n<pre><code>* “EM”: exact match score\n\n\n* “f1”: F1-Score\n\n\n* “top_n_accuracy”: Proportion of predicted answers that match with correct answer\n</code></pre>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>data_dir</strong> (*Path** or *<em>str</em>) – The directory in which the test set can be found</li>\n<li><strong>test_filename</strong> (<em>str</em>) – The name of the file containing the test data in SQuAD format.</li>\n<li><strong>device</strong> (<em>str</em>) – The device on which the tensors should be processed. Choose from “cpu” and “cuda”.</li>\n</ul>\n</li>\n</ul>\n<h4>predict(question: str, documents: List[haystack.database.base.Document], top_k: Optional[int] = None)</h4>\n<p>Use loaded QA model to find answers for a question in the supplied list of Document.</p>\n<p>Returns dictionaries containing answers sorted by (desc.) probability\nExample:\n{‘question’: ‘Who is the father of Arya Stark?’,\n‘answers’: [</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>{‘answer’: ‘Eddard,’,\n‘context’: ” She travels with her father, Eddard, to King’s Landing when he is “,\n‘offset<em>answer</em>start’: 147,\n‘offset<em>answer</em>end’: 154,\n‘probability’: 0.9787139466668613,\n‘score’: None,\n‘document_id’: ‘1337’\n},</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>…</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>]</p>\n</blockquote>\n<p>}</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>question</strong> – question string</li>\n<li><strong>documents</strong> – list of Document in which to search for the answer</li>\n<li><strong>top_k</strong> – the maximum number of answers to return</li>\n</ul>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>dict containing question and answers</p>\n</li>\n</ul>\n<h4>train(data<em>dir: str, train</em>filename: str, dev<em>filename: Optional[str] = None, test</em>file<em>name: Optional[str] = None, use</em>gpu: Optional[bool] = None, batch<em>size: int = 10, n</em>epochs: int = 2, learning<em>rate: float = 1e-05, max</em>seq<em>len: Optional[int] = None, warmup</em>proportion: float = 0.2, dev<em>split: Optional[float] = 0.1, evaluate</em>every: int = 300, save_dir: Optional[str] = None)</h4>\n<p>Fine-tune a model on a QA dataset. Options:</p>\n<ul>\n<li>Take a plain language model (e.g. bert-base-cased) and train it for QA (e.g. on SQuAD data)</li>\n<li>Take a QA model (e.g. deepset/bert-base-cased-squad2) and fine-tune it for your domain (e.g. using your labels collected via the haystack annotation tool)</li>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>data_dir</strong> – Path to directory containing your training data in SQuAD style</li>\n<li><strong>train_filename</strong> – filename of training data</li>\n<li><strong>dev_filename</strong> – filename of dev / eval data</li>\n<li><strong>test<em>file</em>name</strong> – filename of test data</li>\n<li><strong>dev_split</strong> – Instead of specifying a dev_filename you can also specify a ratio (e.g. 0.1) here\nthat get’s split off from training data for eval.</li>\n<li><strong>use_gpu</strong> – Whether to use GPU (if available)</li>\n<li><strong>batch_size</strong> – Number of samples the model receives in one batch for training</li>\n<li><strong>n_epochs</strong> – number of iterations on the whole training data set</li>\n<li><strong>learning_rate</strong> – learning rate of the optimizer</li>\n<li><strong>max<em>seq</em>len</strong> – maximum text length (in tokens). Everything longer gets cut down.</li>\n<li><strong>warmup_proportion</strong> – Proportion of training steps until maximum learning rate is reached.\nUntil that point LR is increasing linearly. After that it’s decreasing again linearly.\nOptions for different schedules are available in FARM.</li>\n<li><strong>evaluate_every</strong> – Evaluate the model every X steps on the hold-out eval dataset</li>\n<li><strong>save_dir</strong> – Path to store the final model</li>\n</ul>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>None</p>\n</li>\n</ul>\n<h2>Transformers</h2>\n<h3>class haystack.reader.transformers.TransformersReader(model: str = 'distilbert-base-uncased-distilled-squad', tokenizer: str = 'distilbert-base-uncased', context<em>window</em>size: int = 30, use<em>gpu: int = 0, n</em>best<em>per</em>passage: int = 2)</h3>\n<p>Bases: <code>haystack.reader.base.BaseReader</code></p>\n<p>Transformer based model for extractive Question Answering using the huggingface’s transformers framework\n(<a href=\"https://github.com/huggingface/transformers\">https://github.com/huggingface/transformers</a>).\nWhile the underlying model can vary (BERT, Roberta, DistilBERT …) the interface remains the same.</p>\n<p>With the reader, you can:</p>\n<pre><code>* directly get predictions via predict()\n</code></pre>\n<h4>_<em>init__(model: str = 'distilbert-base-uncased-distilled-squad', tokenizer: str = 'distilbert-base-uncased', context</em>window<em>size: int = 30, use</em>gpu: int = 0, n<em>best</em>per_passage: int = 2)</h4>\n<p>Load a QA model from Transformers.\nAvailable models include:</p>\n<ul>\n<li>distilbert-base-uncased-distilled-squad</li>\n<li>bert-large-cased-whole-word-masking-finetuned-squad</li>\n<li>bert-large-uncased-whole-word-masking-finetuned-squad</li>\n</ul>\n<p>See <a href=\"https://huggingface.co/models\">https://huggingface.co/models</a> for full list of available QA models</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>model</strong> – name of the model</li>\n<li><strong>tokenizer</strong> – name of the tokenizer (usually the same as model)</li>\n<li><strong>context<em>window</em>size</strong> – num of chars (before and after the answer) to return as “context” for each answer.\nThe context usually helps users to understand if the answer really makes sense.</li>\n<li>\n<p><strong>use_gpu</strong> – &#x3C; 0  -> use cpu</p>\n<blockquote>\n<p>= 0 -> ordinal of the gpu to use</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4>predict(question: str, documents: List[haystack.database.base.Document], top_k: Optional[int] = None)</h4>\n<p>Use loaded QA model to find answers for a question in the supplied list of Document.</p>\n<p>Returns dictionaries containing answers sorted by (desc.) probability\nExample:\n{‘question’: ‘Who is the father of Arya Stark?’,\n‘answers’: [</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>{‘answer’: ‘Eddard,’,\n‘context’: ” She travels with her father, Eddard, to King’s Landing when he is “,\n‘offset<em>answer</em>start’: 147,\n‘offset<em>answer</em>end’: 154,\n‘probability’: 0.9787139466668613,\n‘score’: None,\n‘document_id’: None\n},</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>…</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>]</p>\n</blockquote>\n<p>}</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>question</strong> – question string</li>\n<li><strong>documents</strong> – list of Document in which to search for the answer</li>\n<li><strong>top_k</strong> – the maximum number of answers to return</li>\n</ul>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>dict containing question and answers</p>\n</li>\n</ul>"}},"staticQueryHashes":[]}