{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/retrievermd","result":{"data":{"markdownRemark":{"frontmatter":{"id":"retrievermd","title":"Retriever"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"retrievermd","headings":[{"value":"Retriever","depth":1},{"value":"TF-IDF","depth":2},{"value":"Description","depth":3},{"value":"Initialisation","depth":3},{"value":"BM25 (Recommended)","depth":2},{"value":"Description","depth":3},{"value":"Initialisation","depth":3},{"value":"Dense Passage Retrieval (Recommended)","depth":2},{"value":"Description","depth":3},{"value":"Initialisation","depth":3},{"value":"Embedding Retrieval","depth":2},{"value":"Description","depth":3},{"value":"Initialisation","depth":3},{"value":"Deeper Dive: Dense vs Sparse","depth":2},{"value":"Qualitative Differences","depth":3},{"value":"Indexing","depth":3},{"value":"Terminology","depth":3}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/usage/usage/retriever.md","editPath":"usage/usage/retriever.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Retriever</h1>\n<p>The Retriever is a lightweight filter that can quickly go through the full document store and pass a set of candidate documents to the Reader.\nIt is an tool for sifting out the obvious negative cases, saving the Reader from doing more work than it needs to and speeding up the querying process.</p>\n<p>Recommendations:</p>\n<ul>\n<li>BM25 (sparse)</li>\n<li>Dense Passage Retrieval (dense)</li>\n</ul>\n<!-- _comment: !! Example speedup from slides !! -->\n<!-- _comment: !! Benchmarks !! -->\n<p>Note that not all Retrievers can be paired with every DocumentStore.\nHere are the combinations which are supported:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Memory</th>\n<th>Elasticsearch</th>\n<th>SQL</th>\n<th>FAISS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BM25</td>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n</tr>\n<tr>\n<td>TF-IDF</td>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n</tr>\n<tr>\n<td>Embedding</td>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n</tr>\n<tr>\n<td>DPR</td>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n</tr>\n</tbody>\n</table>\n<h2>TF-IDF</h2>\n<h3>Description</h3>\n<p>TF-IDF is a commonly used baseline for information retrieval that exploits two key intuitions:</p>\n<ul>\n<li>documents that have more lexical overlap with the query are more likely to be relevant</li>\n<li>words that occur in fewer documents are more significant than words that occur in many documents</li>\n</ul>\n<p>Given a query, a tf-idf score is computed for each document as follows:</p>\n<pre><code>score = tf * idf\n</code></pre>\n<p>Where:</p>\n<ul>\n<li><code>tf</code> is how many times words in the query occur in that document.</li>\n<li><code>idf</code> is the inverse of the fraction of documents containing the word.</li>\n</ul>\n<p>In practice, both terms are usually log normalised. If you’d like to learn more about the exact details of the algorithm,\nhave a look at <a href=\"https://www.youtube.com/watch?v=hNXwhF0OZ_o\">this video</a></p>\n<h3>Initialisation</h3>\n<pre><code>document_store = InMemoryDocumentStore()\n...\nretriever = TfidfRetriever(document_store)\n...\nfinder = Finder(reader, retriever)\n</code></pre>\n<h2>BM25 (Recommended)</h2>\n<h3>Description</h3>\n<p>BM25 is a variant of TF-IDF that we recommend you use if you are looking for a retrieval method that does not need a neural network for indexing.\nIt improves upon its predecessor in two main aspects:</p>\n<ul>\n<li>It saturates <code>tf</code> after a set number of occurrences of the given term in the document</li>\n<li>It normalises by document length so that short documents are favoured over long documents if they have the same amount of word overlap with the query</li>\n</ul>\n<h3>Initialisation</h3>\n<pre><code>document_store = ElasticsearchDocumentStore()\n...\nretriever = ElasticsearchRetriever(document_store)\n...\nfinder = Finder(reader, retriever)\n</code></pre>\n<p>See <a href=\"https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables\">this</a> blog post for more details about the algorithm.</p>\n<!-- _comment: !! Diagram showing TFIDF vs BM25 !! -->\n<h2>Dense Passage Retrieval (Recommended)</h2>\n<h3>Description</h3>\n<p><a href=\"https://arxiv.org/abs/2004.04906\">Dense Passage Retrieval</a> is a highly performant retrieval method that calculates relevance using dense representations.\nKey features:</p>\n<ul>\n<li>One BERT base model to encode documents</li>\n<li>One Bert base model to encode queries</li>\n<li>Ranking of documents done by dot product similarity between query and document embeddings</li>\n</ul>\n<!-- _comment: !! Diagram !! -->\n<p>Indexing using DPR is comparatively expensive in terms of required computation since all documents in the database need to be processed through the transformer.\nThe embeddings that are created in this step can be stored in FAISS, a database optimized for vector similarity.\nDPR can also work with the ElasticsearchDocumentStore or the InMemoryDocumentStore.</p>\n<p>There are two design decisions that have made DPR particularly performant.</p>\n<ul>\n<li>Separate encoders for document and query helps since queries are much shorter than documents</li>\n<li>Training with ‘In-batch negatives’ (gold labels are treated as negative examples for other samples in same batch) is highly efficient</li>\n</ul>\n<p>In Haystack, you can simply download the pretrained encoders needed to start using DPR.\nIf you’d like to learn how to set up a DPR based system, have a look at our tutorials.</p>\n<h3>Initialisation</h3>\n<pre><code>document_store = FAISSDocumentStore()\n...\nretriever = DensePassageRetriever(\n    document_store=document_store,\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n)\n...\nfinder = Finder(reader, retriever)\n</code></pre>\n<!-- _comment: !! Training in future? !! -->\n<!-- _comment: !! Talk more about benchmarks, SoTA, results !! -->\n<h2>Embedding Retrieval</h2>\n<h3>Description</h3>\n<p>In Haystack, you also have the option of using a single transformer model to encode document and query.\nOne style of model that is suited to this kind of retrieval is that of <a href=\"https://github.com/UKPLab/sentence-transformers\">Sentence Transformers</a>.\nThese models are trained in Siamese Networks and use triplet loss such that they learn to embed similar sentences near to each other in a shared embedding space.</p>\n<p>They are particular suited to cases where your query input is similar in style to that of the documents in your database\ni.e. when you are searching for most similar documents.\nThis is not inherently suited to query based search where the length, language and format of the query usually significantly differs from the searched for text.</p>\n<h3>Initialisation</h3>\n<pre><code>document_store = ElasticsearchDocumentStore()\n...\nretriever = EmbeddingRetriever(document_store=document_store,\n                               embedding_model=\"deepset/sentence_bert\")\n...\nfinder = Finder(reader, retriever)\n</code></pre>\n<h2>Deeper Dive: Dense vs Sparse</h2>\n<p>Broadly speaking, retrieval methods can be split into two categories: <strong>dense</strong> and <strong>sparse</strong>.</p>\n<p><strong>Sparse</strong> methods, like TF-IDF and BM25, operate by looking for shared keywords between the document and query.\nThey are:</p>\n<ul>\n<li>simple but effective</li>\n<li>don’t need to be trained</li>\n<li>work on any language</li>\n</ul>\n<p>More recently, <strong>dense</strong> approaches such as Dense Passage Retrieval (DPR) have shown even better performance than their sparse counter parts.\nThese methods embed both document and query into a shared embedding space using deep neural networks\nand the top candidates are the nearest neighbour documents to the query.\nThey are:</p>\n<ul>\n<li>powerful but computationally more expensive especially during indexing</li>\n<li>trained using labelled datasets</li>\n<li>language specific</li>\n</ul>\n<h3>Qualitative Differences</h3>\n<p>Between these two types there are also some qualitative differences too.\nFor example, sparse methods treat text as a bag-of-words meaning that they <strong>do not take word order and syntax into account</strong>,\nwhile the latest generation of dense methods use transformer based encoders\nwhich are designed to be <strong>sensitive</strong> to these factors.</p>\n<p>Also dense methods are very capable of building strong semantic representations of text,\nbut they <strong>struggle when encountering out-of-vocabulary</strong> words such as new names.\nBy contrast, sparse methods don’t need to learn representations of words,\nthey only care about whether they are present or absent in the text.\nAs such, <strong>they handle out-of-vocabulary words with no problem</strong>.</p>\n<!-- _comment: !! Show example from DPR paper? !! -->\n<h3>Indexing</h3>\n<p>Dense methods perform indexing by processing all the documents through a neural network and storing the resulting vectors.\nThis is a much more expensive operation than the creation of the inverted-index in sparse methods\nand will require significant computational power and time.</p>\n<!-- _comment: !!See their individual sections (!! link !!) for more details on this point. Benchmarks too !! -->\n<h3>Terminology</h3>\n<!-- _comment: !! Diagram of what a sparse vector looks like vs dense vector. !! -->\n<!-- _comment: !! This section should be turned into something more like a side note !! -->\n<p>The terms <strong>dense</strong> and <strong>sparse</strong> refer to the representations that the algorithms build for each document and query.\n<strong>Sparse</strong> methods characterise texts using vectors with one dimension corresponding to each word in the vocabulary.\nDimensions will be zero if the word is absent and non-zero if it is present.\nSince most documents contain only a small subset of the full vocabulary,\nthese vectors are considered sparse since non-zero values are few and far between.</p>\n<p><strong>Dense</strong> methods, by contrast, pass text as input into neural network encoders\nand represent text in a vector of a manually defined size (usually 768).\nThough individual dimensions are not mapped to any corresponding vocabulary or linguistic feature,\neach dimension encodes some information about the text.\nThere are rarely 0s in these vectors hence their relative density.</p>"}},"staticQueryHashes":[]}