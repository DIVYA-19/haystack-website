{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/apiretrievermd","result":{"data":{"markdownRemark":{"frontmatter":{"id":"apiretrievermd","title":"Retriever"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"apiretrievermd","headings":[{"value":"Retriever","depth":1},{"value":"Base","depth":2},{"value":"class haystack.retriever.base.BaseRetriever()","depth":3},{"value":"Dense","depth":2},{"value":"class haystack.retriever.dense.DensePassageRetriever(documentstore: haystack.database.base.BaseDocumentStore, embeddingmodel: str, usegpu: bool = True, batchsize: int = 16, dolowercase: bool = False, use_amp: str = None)","depth":3},{"value":"remote model from FAIR","depth":1},{"value":"or from local path","depth":1},{"value":"class haystack.retriever.dense.EmbeddingRetriever(documentstore: haystack.database.elasticsearch.ElasticsearchDocumentStore, embeddingmodel: str, usegpu: bool = True, modelformat: str = 'farm', poolingstrategy: str = 'reducemean', embextractionlayer: int = - 1)","depth":3},{"value":"Sparse","depth":2},{"value":"class haystack.retriever.sparse.ElasticsearchFilterOnlyRetriever(documentstore: haystack.database.elasticsearch.ElasticsearchDocumentStore, customquery: str = None)","depth":3},{"value":"class haystack.retriever.sparse.ElasticsearchRetriever(documentstore: haystack.database.elasticsearch.ElasticsearchDocumentStore, customquery: str = None)","depth":3},{"value":"class haystack.retriever.sparse.Paragraph(paragraphid, documentid, text, meta)","depth":3},{"value":"class haystack.retriever.sparse.TfidfRetriever(document_store: haystack.database.base.BaseDocumentStore)","depth":3}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/api/api/retriever.md","editPath":"api/api/retriever.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Retriever</h1>\n<h2>Base</h2>\n<h3>class haystack.retriever.base.BaseRetriever()</h3>\n<p>Bases: <code>abc.ABC</code></p>\n<h2>Dense</h2>\n<h3>class haystack.retriever.dense.DensePassageRetriever(document<em>store: haystack.database.base.BaseDocumentStore, embedding</em>model: str, use<em>gpu: bool = True, batch</em>size: int = 16, do<em>lower</em>case: bool = False, use_amp: str = None)</h3>\n<p>Bases: <code>haystack.retriever.base.BaseRetriever</code></p>\n<p>Retriever that uses a bi-encoder (one transformer for query, one transformer for passage).\nSee the original paper for more details:\nKarpukhin, Vladimir, et al. (2020): “Dense Passage Retrieval for Open-Domain Question Answering.”\n(<a href=\"https://arxiv.org/abs/2004.04906\">https://arxiv.org/abs/2004.04906</a>).</p>\n<h4>_<em>init__(document</em>store: haystack.database.base.BaseDocumentStore, embedding<em>model: str, use</em>gpu: bool = True, batch<em>size: int = 16, do</em>lower<em>case: bool = False, use</em>amp: str = None)</h4>\n<p>Init the Retriever incl. the two encoder models from a local or remote model checkpoint.\nThe checkpoint format matches the one of the original author’s in the repository (<a href=\"https://github.com/facebookresearch/DPR\">https://github.com/facebookresearch/DPR</a>)\nSee their readme for manual download instructions: <a href=\"https://github.com/facebookresearch/DPR#resources--data-formats\">https://github.com/facebookresearch/DPR#resources–data-formats</a></p>\n<ul>\n<li>\n<p><strong>Example</strong></p>\n<h1>remote model from FAIR</h1>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>DensePassageRetriever(document<em>store=your</em>doc<em>store, embedding</em>model=”dpr-bert-base-nq”, use_gpu=True)</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<h1>or from local path</h1>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>DensePassageRetriever(document<em>store=your</em>doc<em>store, embedding</em>model=”some<em>path/ber-base-encoder.cp”, use</em>gpu=True)</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</li>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>document_store</strong> – An instance of DocumentStore from which to retrieve documents.</li>\n<li><strong>embedding_model</strong> – Local path or remote name of model checkpoint. The format equals the\none used by original author’s in <a href=\"https://github.com/facebookresearch/DPR\">https://github.com/facebookresearch/DPR</a>.\nCurrently available remote names: “dpr-bert-base-nq”</li>\n<li><strong>use_gpu</strong> – Whether to use gpu or not</li>\n<li><strong>batch_size</strong> – Number of questions or passages to encode at once</li>\n<li><strong>do<em>lower</em>case</strong> – Whether to lower case the text input in the tokenizer</li>\n<li><strong>encoder<em>model</em>type</strong> – </li>\n<li><strong>use_amp</strong> – Whether to use Automatix Mixed Precision optimization from apex’s to improve speed and memory consumption.</li>\n<li><strong>use_amp</strong> – Optional usage of Automatix Mixed Precision optimization from apex’s to improve speed and memory consumption.\nChoose None or AMP optimization level:</li>\n</ul>\n<blockquote>\n<pre><code>* None -> Not using amp at all\n</code></pre>\n</blockquote>\n<blockquote>\n<pre><code>* ’O0’ -> Regular FP32\n</code></pre>\n</blockquote>\n<blockquote>\n<pre><code>* ’O1’ -> Mixed Precision (recommended, if optimization wanted)\n</code></pre>\n</blockquote>\n</li>\n</ul>\n<h4>embed_passages(texts: List[str])</h4>\n<p>Create embeddings for a list of passages using the passage encoder</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<p><strong>texts</strong> – passage to embed</p>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>embeddings, one per input passage</p>\n</li>\n</ul>\n<h4>embed_queries(texts: List[str])</h4>\n<p>Create embeddings for a list of queries using the query encoder</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<p><strong>texts</strong> – queries to embed</p>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>embeddings, one per input queries</p>\n</li>\n</ul>\n<h3>class haystack.retriever.dense.EmbeddingRetriever(document<em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, embedding</em>model: str, use<em>gpu: bool = True, model</em>format: str = 'farm', pooling<em>strategy: str = 'reduce</em>mean', emb<em>extraction</em>layer: int = - 1)</h3>\n<p>Bases: <code>haystack.retriever.base.BaseRetriever</code></p>\n<h4>_<em>init__(document</em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, embedding<em>model: str, use</em>gpu: bool = True, model<em>format: str = 'farm', pooling</em>strategy: str = 'reduce<em>mean', emb</em>extraction_layer: int = - 1)</h4>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>document_store</strong> – An instance of DocumentStore from which to retrieve documents.</li>\n<li><strong>embedding_model</strong> – Local path or name of model in Hugging Face’s model hub. Example: ‘deepset/sentence_bert’</li>\n<li><strong>use_gpu</strong> – Whether to use gpu or not</li>\n<li><strong>model_format</strong> – Name of framework that was used for saving the model. Options: ‘farm’, ‘transformers’, ‘sentence_transformers’</li>\n<li><strong>pooling_strategy</strong> – Strategy for combining the embeddings from the model (for farm / transformers models only).\nOptions: ‘cls<em>token’ (sentence vector), ‘reduce</em>mean’ (sentence vector),\nreduce<em>max (sentence vector), ‘per</em>token’ (individual token vectors)</li>\n<li><strong>emb<em>extraction</em>layer</strong> – Number of layer from which the embeddings shall be extracted (for farm / transformers models only).\nDefault: -1 (very last layer).</li>\n</ul>\n</li>\n</ul>\n<h4>embed(texts: Union[List[str], str])</h4>\n<p>Create embeddings for each text in a list of texts using the retrievers model (self.embedding_model)\n:param texts: texts to embed\n:return: list of embeddings (one per input text). Each embedding is a list of floats.</p>\n<h4>embed_passages(texts: List[str])</h4>\n<p>Create embeddings for a list of passages. For this Retriever type: The same as calling .embed()</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<p><strong>texts</strong> – passage to embed</p>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>embeddings, one per input passage</p>\n</li>\n</ul>\n<h4>embed_queries(texts: List[str])</h4>\n<p>Create embeddings for a list of queries. For this Retriever type: The same as calling .embed()</p>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<p><strong>texts</strong> – queries to embed</p>\n</li>\n<li>\n<p><strong>Returns</strong></p>\n<p>embeddings, one per input queries</p>\n</li>\n</ul>\n<h2>Sparse</h2>\n<h3>class haystack.retriever.sparse.ElasticsearchFilterOnlyRetriever(document<em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, custom</em>query: str = None)</h3>\n<p>Bases: <code>haystack.retriever.sparse.ElasticsearchRetriever</code></p>\n<p>Naive “Retriever” that returns all documents that match the given filters. No impact of query at all.\nHelpful for benchmarking, testing and if you want to do QA on small documents without an “active” retriever.</p>\n<h3>class haystack.retriever.sparse.ElasticsearchRetriever(document<em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, custom</em>query: str = None)</h3>\n<p>Bases: <code>haystack.retriever.base.BaseRetriever</code></p>\n<h4>_<em>init__(document</em>store: haystack.database.elasticsearch.ElasticsearchDocumentStore, custom_query: str = None)</h4>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>document_store</strong> – an instance of a DocumentStore to retrieve documents from.</li>\n<li><strong>custom_query</strong> – query string as per Elasticsearch DSL with a mandatory question placeholder($question).</li>\n</ul>\n<blockquote>\n<p>Optionally, ES filter clause can be added where the values of terms are placeholders\nthat get substituted during runtime. The placeholder(${filter<em>name</em>1}, ${filter<em>name</em>2}..)\nnames must match with the filters dict supplied in self.retrieve().</p>\n</blockquote>\n<blockquote>\n<p>An example custom_query:</p>\n</blockquote>\n<p>{</p>\n<pre><code>“size”: 10,\n“query”: {\n\n> ”bool”: {\n\n>     “should”: [{“multi_match”: {\n\n>         “query”: “${question}”,                 // mandatory $question placeholder\n>         “type”: “most_fields”,\n>         “fields”: [“text”, “title”]}}],\n\n>     ”filter”: [                                 // optional custom filters\n\n>         {“terms”: {“year”: “${years}”}},\n>         {“terms”: {“quarter”: “${quarters}”}},\n>         {“range”: {“date”: {“gte”: “${date}”}}}\n>         ],\n\n> }\n\n},\n</code></pre>\n<p>}</p>\n<blockquote>\n<p>For this custom_query, a sample retrieve() could be:\nself.retrieve(query=”Why did the revenue increase?”,</p>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>filters={“years”: [“2019”], “quarters”: [“Q1”, “Q2”]})</p>\n</blockquote>\n</blockquote>\n</li>\n</ul>\n<h4>eval(label<em>index: str = 'feedback', doc</em>index: str = 'eval<em>document', label</em>origin: str = 'gold<em>label', top</em>k: int = 10)</h4>\n<p>Performs evaluation on the Retriever.\nRetriever is evaluated based on whether it finds the correct document given the question string and at which\nposition in the ranking of documents the correct document is.</p>\n<p>Returns a dict containing the following metrics:</p>\n<pre><code>* “recall”: Proportion of questions for which correct document is among retrieved documents\n\n\n* “mean avg precision”: Mean of average precision for each question. Rewards retrievers that give relevant\ndocuments a higher rank.\n</code></pre>\n<ul>\n<li>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><strong>label_index</strong> – Index/Table in DocumentStore where labeled questions are stored</li>\n<li><strong>doc_index</strong> – Index/Table in DocumentStore where documents that are used for evaluation are stored</li>\n<li><strong>top_k</strong> – How many documents to return per question</li>\n</ul>\n</li>\n</ul>\n<h3>class haystack.retriever.sparse.Paragraph(paragraph<em>id, document</em>id, text, meta)</h3>\n<p>Bases: <code>tuple</code></p>\n<h4>document_id()</h4>\n<p>Alias for field number 1</p>\n<h4>meta()</h4>\n<p>Alias for field number 3</p>\n<h4>paragraph_id()</h4>\n<p>Alias for field number 0</p>\n<h4>text()</h4>\n<p>Alias for field number 2</p>\n<h3>class haystack.retriever.sparse.TfidfRetriever(document_store: haystack.database.base.BaseDocumentStore)</h3>\n<p>Bases: <code>haystack.retriever.base.BaseRetriever</code></p>\n<p>Read all documents from a SQL backend.</p>\n<p>Split documents into smaller units (eg, paragraphs or pages) to reduce the\ncomputations when text is passed on to a Reader for QA.</p>\n<p>It uses sklearn’s TfidfVectorizer to compute a tf-idf matrix.</p>\n<h4>_<em>init__(document</em>store: haystack.database.base.BaseDocumentStore)</h4>\n<p>Initialize self.  See help(type(self)) for accurate signature.</p>"}},"staticQueryHashes":[]}