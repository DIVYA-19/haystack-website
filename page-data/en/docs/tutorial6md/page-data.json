{"componentChunkName":"component---src-templates-doc-template-js","path":"/en/docs/tutorial6md","result":{"data":{"markdownRemark":{"frontmatter":{"id":"tutorial6md","title":"Tutorial 6"}},"allFile":{"edges":[{"node":{"relativeDirectory":"layout","childLayoutJson":{"layout":{"header":{"quick":"Quick Start","benchmarks":"Benchmarks","why":"Why Haystack","gui":"Admin","tutorials":"Tutorials","solution":"Scenarios","about":"About Haystack","doc":"Docs","blog":"Blog","try":"Try","loading":"Loading...","noresult":"No Result","tutorial":"Tutorial","search":"Search","bootcamp":"Bootcamp"}}}}}]}},"pageContext":{"locale":"en","old":"tutorial6md","headings":[{"value":"Better retrieval via “Dense Passage Retrieval”","depth":1},{"value":"Importance of Retrievers","depth":2},{"value":"Different types of Retrievers","depth":2},{"value":"Sparse","depth":3},{"value":"Dense","depth":3},{"value":"“Dense Passage Retrieval”","depth":2},{"value":"Prepare environment","depth":2},{"value":"Colab: Enable the GPU runtime","depth":3},{"value":"Document Store","depth":2},{"value":"Cleaning & indexing documents","depth":2},{"value":"Initalize Retriever, Reader, & Finder","depth":2},{"value":"Retriever","depth":3},{"value":"Reader","depth":3},{"value":"Finder","depth":3},{"value":"Voilà! Ask a question!","depth":2}],"fileAbsolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/tutorials/tutorials/6.md","editPath":"tutorials/tutorials/6.rst","allMenus":[{"lang":"en","menuList":[{"id":"usage_haystack","title":"Usage","label1":"","label2":"","label3":"","order":0,"isMenu":true},{"id":"intromd","title":"What is Haystack","label1":"usage_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"get_startedmd","title":"Get Started","label1":"usage_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"databasemd","title":"Document Store","label1":"usage_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"retrievermd","title":"Retriever","label1":"usage_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"readermd","title":"Reader","label1":"usage_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"domain_adaptionmd","title":"Domain Adaption","label1":"usage_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"termsmd","title":"Glossary","label1":"usage_haystack","label2":"","label3":"","order":6,"isMenu":null},{"id":"tutorials_haystack","title":"Tutorials","label1":"","label2":"","label3":"","order":1,"isMenu":true},{"id":"tutorial1md","title":"Task: Question Answering for Game of Thrones","label1":"tutorials_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"tutorial2md","title":"Fine-tuning a model on your own data","label1":"tutorials_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"tutorial3md","title":"Task: Build a Question Answering pipeline without Elasticsearch","label1":"tutorials_haystack","label2":"","label3":"","order":2,"isMenu":null},{"id":"tutorial4md","title":"FAQ-Style QA: Utilizing existing FAQs for Question Answering","label1":"tutorials_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"tutorial5md","title":"Evaluation","label1":"tutorials_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"tutorial6md","title":"Better retrieval via Dense Passage Retrieval","label1":"tutorials_haystack","label2":"","label3":"","order":5,"isMenu":null},{"id":"api_haystack","title":"API","label1":"","label2":"","label3":"","order":2,"isMenu":true},{"id":"apidatabasemd","title":"Database","label1":"api_haystack","label2":"","label3":"","order":0,"isMenu":null},{"id":"apiretrievermd","title":"Retriever","label1":"api_haystack","label2":"","label3":"","order":1,"isMenu":null},{"id":"apireadermd","title":"Reader","label1":"api_haystack","label2":"api_haystack","label3":"","order":2,"isMenu":null},{"id":"apiindexingmd","title":"Indexing","label1":"api_haystack","label2":"","label3":"","order":3,"isMenu":null},{"id":"rest_apimd","title":"Rest API","label1":"api_haystack","label2":"","label3":"","order":4,"isMenu":null},{"id":"file_convertersmd","title":"File Converters","label1":"api_haystack","label2":"","label3":"","order":5,"isMenu":null}],"absolutePath":"/home/markus/Documents/git/haystack-io/src/pages/docs/site/en/menuStructure/menu.json"}],"newHtml":"<h1>Better retrieval via “Dense Passage Retrieval”</h1>\n<h2>Importance of Retrievers</h2>\n<p>The Retriever has a huge impact on the performance of our overall search\npipeline.</p>\n<h2>Different types of Retrievers</h2>\n<h3>Sparse</h3>\n<p>Family of algorithms based on counting the occurences of words\n(bag-of-words) resulting in very sparse vectors with length = vocab\nsize.</p>\n<p><strong>Examples</strong>: BM25, TF-IDF</p>\n<p><strong>Pros</strong>: Simple, fast, well explainable</p>\n<p><strong>Cons</strong>: Relies on exact keyword matches between query and text</p>\n<h3>Dense</h3>\n<p>These retrievers use neural network models to create “dense” embedding\nvectors. Within this family there are two different approaches:</p>\n<ol>\n<li>Single encoder: Use a <strong>single model</strong> to embed both query and\npassage.</li>\n<li>Dual-encoder: Use <strong>two models</strong>, one to embed the query and one to\nembed the passage</li>\n</ol>\n<p>Recent work suggests that dual encoders work better, likely because they\ncan deal better with the different nature of query and passage (length,\nstyle, syntax …).</p>\n<p><strong>Examples</strong>: REALM, DPR, Sentence-Transformers</p>\n<p><strong>Pros</strong>: Captures semantinc similarity instead of “word matches”\n(e.g. synonyms, related topics …)</p>\n<p><strong>Cons</strong>: Computationally more heavy, initial training of model</p>\n<h2>“Dense Passage Retrieval”</h2>\n<p>In this Tutorial, we want to highlight one “Dense Dual-Encoder” called\nDense Passage Retriever. It was introdoced by Karpukhin et al. (2020,\n<a href=\"https://arxiv.org/abs/2004.04906\">https://arxiv.org/abs/2004.04906</a>.</p>\n<p>Original Abstract:</p>\n<p><em>“Open-domain question answering relies on efficient passage retrieval\nto select candidate contexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de facto method. In this work,\nwe show that retrieval can be practically implemented using dense\nrepresentations alone, where embeddings are learned from a small number\nof questions and passages by a simple dual-encoder framework. When\nevaluated on a wide range of open-domain QA datasets, our dense\nretriever outperforms a strong Lucene-BM25 system largely by 9%-19%\nabsolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple\nopen-domain QA benchmarks.”</em></p>\n<p>Paper: <a href=\"https://arxiv.org/abs/2004.04906\">https://arxiv.org/abs/2004.04906</a>Original Code: <a href=\"https://fburl.com/qa-dpr\">https://fburl.com/qa-dpr</a><em>Use this</em>\n<a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb\">link</a>\n<em>to open the notebook in Google Colab.</em></p>\n<h2>Prepare environment</h2>\n<h3>Colab: Enable the GPU runtime</h3>\n<p>Make sure you enable the GPU runtime to experience decent speed in\nthis tutorial.<strong>Runtime -> Change Runtime type -> Hardware accelerator -> GPU</strong></p>\n<pre><code># Make sure you have a GPU running\n!nvidia-smi\n</code></pre>\n<pre><code># Install the latest release of Haystack in your own environment\n#! pip install farm-haystack\n\n# Install the latest master of Haystack and install the version of torch that works with the colab GPUs\n!pip install git+https://github.com/deepset-ai/haystack.git\n!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<pre><code>from haystack import Finder\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers\n</code></pre>\n<h2>Document Store</h2>\n<p>FAISS is a library for efficient similarity search on a cluster of dense\nvectors. The <code>FAISSDocumentStore</code> uses a SQL(SQLite in-memory be\ndefault) database under-the-hood to store the document text and other\nmeta data. The vector embeddings of the text are indexed on a FAISS\nIndex that later is queried for searching answers.</p>\n<pre><code>from haystack.database.faiss import FAISSDocumentStore\n\ndocument_store = FAISSDocumentStore()\n</code></pre>\n<h2>Cleaning &#x26; indexing documents</h2>\n<p>Similarly to the previous tutorials, we download, convert and index some\nGame of Thrones articles to our DocumentStore</p>\n<pre><code># Let's first get some files that we want to use\ndoc_dir = \"data/article_txt_got\"\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# Convert files to dicts\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n\n# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(dicts)\n</code></pre>\n<h2>Initalize Retriever, Reader, &#x26; Finder</h2>\n<h3>Retriever</h3>\n<p><strong>Here:</strong> We use a <code>DensePassageRetriever</code></p>\n<p><strong>Alternatives:</strong></p>\n<ul>\n<li>The <code>ElasticsearchRetriever</code>with custom queries (e.g. boosting)\nand filters</li>\n<li>Use <code>EmbeddingRetriever</code> to find candidate documents based on the\nsimilarity of embeddings (e.g. created via Sentence-BERT)</li>\n<li>Use <code>TfidfRetriever</code> in combination with a SQL or InMemory Document\nstore for simple prototyping and debugging</li>\n</ul>\n<pre><code>from haystack.retriever.dense import DensePassageRetriever\nretriever = DensePassageRetriever(document_store=document_store,\n                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n                                  use_gpu=True,\n                                  embed_title=True,\n                                  max_seq_len=256,\n                                  batch_size=16,\n                                  remove_sep_tok_from_untitled_passages=True)\n# Important:\n# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n# previously indexed documents and update their embedding representation.\n# While this can be a time consuming operation (depending on corpus size), it only needs to be done once.\n# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\ndocument_store.update_embeddings(retriever)\n</code></pre>\n<h3>Reader</h3>\n<p>Similar to previous Tutorials we now initalize our reader.</p>\n<p>Here we use a FARMReader with the <em>deepset/roberta-base-squad2</em> model\n(see: <a href=\"https://huggingface.co/deepset/roberta-base-squad2\">https://huggingface.co/deepset/roberta-base-squad2</a>)</p>\n<h4>FARMReader</h4>\n<pre><code># Load a  local model or any of the QA models on\n# Hugging Face's model hub (https://huggingface.co/models)\n\nreader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n</code></pre>\n<h3>Finder</h3>\n<p>The Finder sticks together reader and retriever in a pipeline to answer\nour actual questions.</p>\n<pre><code>finder = Finder(reader, retriever)\n</code></pre>\n<h2>Voilà! Ask a question!</h2>\n<pre><code># You can configure how many candidates the reader and retriever shall return\n# The higher top_k_retriever, the better (but also the slower) your answers.\nprediction = finder.get_answers(question=\"Who created the Dothraki vocabulary?\", top_k_retriever=10, top_k_reader=5)\n\n#prediction = finder.get_answers(question=\"Who is the father of Arya Stark?\", top_k_retriever=10, top_k_reader=5)\n#prediction = finder.get_answers(question=\"Who is the sister of Sansa?\", top_k_retriever=10, top_k_reader=5)\n</code></pre>\n<pre><code>print_answers(prediction, details=\"minimal\")\n</code></pre>"}},"staticQueryHashes":[]}